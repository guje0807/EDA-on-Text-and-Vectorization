{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Akash\n",
      "[nltk_data]     Guje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk as nt\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "nt.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myString = \"This is a paragraph. It should split at the end of sentence marker, such as a period. It can tell that the period in Mr. John is not an end. Run it!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function tokenizes a paragraph with a delimiter of full stop \n",
    "tokenized_sentence = sent_tokenize ( myString )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a paragraph.', 'It should split at the end of sentence marker, such as a period.', 'It can tell that the period in Mr. John is not an end.', 'Run it!']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'Akash!!', 'Nice', 'to', 'meet', 'you:)']\n"
     ]
    }
   ],
   "source": [
    "a = \"This is Akash!! Nice to meet you:)\"\n",
    "print(a.split())    # Split function tokenizes the sentence by whitespace delimiter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'Akash', '!', '!', 'Nice', 'to', 'meet', 'you', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "# from nltk.tokenize import word_tokenize, regexp_tokenize \n",
    "\n",
    "print(word_tokenize(a))  # This function tokenizes the puntuation marks also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regexp_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-eb1d073d419d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregexp_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#The \\w+ is an indication that we need all the words and digits to be in the token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# '\\d+' this pattern returns the digits from the sentence when passed through regex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'regexp_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(a,'\\w+')) #The \\w+ is an indication that we need all the words and digits to be in the token\n",
    "\n",
    "# '\\d+' this pattern returns the digits from the sentence when passed through regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,LancasterStemmer, SnowballStemmer\n",
    "\n",
    "porter = PorterStemmer()    # This objects gets the root form a word using suffix stripping. \n",
    "print(porter.stem('Reading'))\n",
    "\n",
    "lac = LancasterStemmer()\n",
    "print(lac.stem('troubled'))\n",
    "\n",
    "snw = SnowballStemmer(\"english\")\n",
    "\n",
    "print(snw.stem('Sleeping'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "nt.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()   # this considers the parts of speech and the tense to get the root form \n",
    "\n",
    "s = \"I was driving a car when it was raining\"\n",
    "\n",
    "for i in s.split():\n",
    "    print(lem.lemmatize(i) , porter.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nt.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = \"Shiva Trilogy is a book about indian civilization during the time of shiva\"\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "tkn = [i for i in b.split() if i not in sw]\n",
    "\n",
    "print(tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(sw[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPLACING AND CORRECTING WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "MyString = \"My Name is Akash, I am 24 years old living in toronto since 4 months\"\n",
    "\n",
    "s = re.sub(r'\\d+','',MyString)    #This removes the numbers from the string\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Summers', 'NNS'), ('are', 'VBP'), ('for', 'IN'), ('parasol', 'NN'), ('Winter', 'NNP'), ('are', 'VBP'), ('for', 'IN'), ('garlosh', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Akash Guje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "nt.download('averaged_perceptron_tagger')\n",
    "\n",
    "c = \"Summers are for parasol, Winter are for garlosh\"\n",
    "d = TextBlob(c)\n",
    "\n",
    "print(d.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Akash\n",
      "[nltk_data]     Guje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Akash\n",
      "[nltk_data]     Guje\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "nt.download('maxent_ne_chunker') \n",
    "nt.download('words')\n",
    "\n",
    "e = \"My Name is Akash. I am Living in Toronto, Canada since January!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  My/PRP$\n",
      "  Name/NN\n",
      "  is/VBZ\n",
      "  (PERSON Akash/NNP)\n",
      "  ./.\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  Living/VBG\n",
      "  in/IN\n",
      "  (GPE Toronto/NNP)\n",
      "  ,/,\n",
      "  (GPE Canada/NNP)\n",
      "  since/IN\n",
      "  January/NNP\n",
      "  !/.\n",
      "  !/.)\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunk(pos_tag(word_tokenize(e))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINDING SYNONYMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('happy.a.01'), Synset('felicitous.s.02'), Synset('glad.s.02'), Synset('happy.s.04')]\n"
     ]
    }
   ],
   "source": [
    "from nltk . corpus import wordnet\n",
    "\n",
    "a = wordnet.synsets(\"Happy\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy [Lemma('unhappy.a.01.unhappy')]\n",
      "\n",
      "felicitous []\n",
      "\n",
      "happy []\n",
      "\n",
      "glad []\n",
      "\n",
      "happy []\n",
      "\n",
      "happy []\n",
      "\n",
      "well-chosen []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in a:\n",
    "    for lem in s.lemmas():\n",
    "        print(lem.name(),lem.antonyms())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('(.)\\\\1{2,}')\n",
      "acctive\n"
     ]
    }
   ],
   "source": [
    "def remove_lengthening ( text ): \n",
    "    patt = re . compile ( r\"(.)\\1{2,}\" ) \n",
    "    print(patt)\n",
    "    return patt . sub ( r\"\\1\\1\" ,text )\n",
    "print ( remove_lengthening ( \"accccctive\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Entertainment', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import suggest\n",
    "print(suggest('Entertainmant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
